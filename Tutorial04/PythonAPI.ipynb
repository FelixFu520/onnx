{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python API\n",
    "### 1、Loading an ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonAPI.ipynb  resources\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'onnx.onnx_ml_pb2.ModelProto'> 139717098346736\n",
      "ir_version: 4\n",
      "producer_name: \"skl2onnx\"\n",
      "producer_version: \"1.7.0\"\n",
      "domain: \"ai.onnx\"\n",
      "model_version: 0\n",
      "doc_string: \"\"\n",
      "graph {\n",
      "  node {\n",
      "    input: \"float_input\"\n",
      "    output: \"label\"\n",
      "    output: \"probability_tensor\"\n",
      "    name: \"LinearClassifier\"\n",
      "    op_type: \"LinearClassifier\"\n",
      "    attribute {\n",
      "      name: \"classlabels_ints\"\n",
      "      ints: 0\n",
      "      ints: 1\n",
      "      ints: 2\n",
      "      type: INTS\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"coefficients\"\n",
      "      floats: -0.5013933181762695\n",
      "      floats: 0.8189675807952881\n",
      "      floats: -2.254585027694702\n",
      "      floats: -0.9972120523452759\n",
      "      floats: 0.5146268606185913\n",
      "      floats: -0.4061462879180908\n",
      "      floats: -0.23364509642124176\n",
      "      floats: -0.7229060530662537\n",
      "      floats: -0.013233542442321777\n",
      "      floats: -0.41282132267951965\n",
      "      floats: 2.488229990005493\n",
      "      floats: 1.7201180458068848\n",
      "      type: FLOATS\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"intercepts\"\n",
      "      floats: 9.711934089660645\n",
      "      floats: 2.2038588523864746\n",
      "      floats: -11.915793418884277\n",
      "      type: FLOATS\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"multi_class\"\n",
      "      i: 1\n",
      "      type: INT\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"post_transform\"\n",
      "      s: \"SOFTMAX\"\n",
      "      type: STRING\n",
      "    }\n",
      "    domain: \"ai.onnx.ml\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"probability_tensor\"\n",
      "    output: \"probabilities\"\n",
      "    name: \"Normalizer\"\n",
      "    op_type: \"Normalizer\"\n",
      "    attribute {\n",
      "      name: \"norm\"\n",
      "      s: \"L1\"\n",
      "      type: STRING\n",
      "    }\n",
      "    domain: \"ai.onnx.ml\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"label\"\n",
      "    output: \"output_label\"\n",
      "    name: \"Cast\"\n",
      "    op_type: \"Cast\"\n",
      "    attribute {\n",
      "      name: \"to\"\n",
      "      i: 7\n",
      "      type: INT\n",
      "    }\n",
      "    domain: \"\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"probabilities\"\n",
      "    output: \"output_probability\"\n",
      "    name: \"ZipMap\"\n",
      "    op_type: \"ZipMap\"\n",
      "    attribute {\n",
      "      name: \"classlabels_int64s\"\n",
      "      ints: 0\n",
      "      ints: 1\n",
      "      ints: 2\n",
      "      type: INTS\n",
      "    }\n",
      "    domain: \"ai.onnx.ml\"\n",
      "  }\n",
      "  name: \"f16ba37543fa487fbce0f9400d714344\"\n",
      "  input {\n",
      "    name: \"float_input\"\n",
      "    type {\n",
      "      tensor_type {\n",
      "        elem_type: 1\n",
      "        shape {\n",
      "          dim {\n",
      "          }\n",
      "          dim {\n",
      "            dim_value: 4\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  output {\n",
      "    name: \"output_label\"\n",
      "    type {\n",
      "      tensor_type {\n",
      "        elem_type: 7\n",
      "        shape {\n",
      "          dim {\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  output {\n",
      "    name: \"output_probability\"\n",
      "    type {\n",
      "      sequence_type {\n",
      "        elem_type {\n",
      "          map_type {\n",
      "            key_type: 7\n",
      "            value_type {\n",
      "              tensor_type {\n",
      "                elem_type: 1\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "opset_import {\n",
      "  domain: \"\"\n",
      "  version: 9\n",
      "}\n",
      "opset_import {\n",
      "  domain: \"ai.onnx.ml\"\n",
      "  version: 1\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "\n",
    "import onnx\n",
    "\n",
    "import os\n",
    "\n",
    "onnx_model = onnx.load(os.path.join('resources', 'logreg_iris.onnx')) # Tutorial01中的ONNX\n",
    "print(type(onnx_model), id(onnx_model))\n",
    "print(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、Loading an ONNX Model with External Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Default] If the external data is under the same directory of the model, simply use onnx.load()   \n",
    "```python\n",
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load('path/to/the/model.onnx')\n",
    "\n",
    "```\n",
    "\n",
    "If the external data is under another directory, use load_external_data_for_model() to specify the directory path and load after using onnx.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx.external_data_helper import load_external_data_for_model\n",
    "\n",
    "onnx_model = onnx.load(os.path.join('resources', 'alexnet.onnx'), load_external_data=False)\n",
    "load_external_data_for_model(onnx_model, '../unet.onnx')\n",
    "# Then the onnx_model has loaded the external data from the specific directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、Saving an ONNX Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is saved.\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: load the old model\n",
    "old_model_path = os.path.join('resources', 'alexnet.onnx')\n",
    "onnx_model = onnx.load(old_model_path)\n",
    "\n",
    "# Preprocessing: get the path to the saved model\n",
    "new_model_path = os.path.join('./', 'alexnet.onnx')\n",
    "\n",
    "# Save the ONNX model\n",
    "onnx.save(onnx_model, new_model_path)\n",
    "\n",
    "print('The model is saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、Manipulating TensorProto and Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Numpy array:\n",
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import numpy\n",
    "import onnx\n",
    "import os\n",
    "from onnx import numpy_helper\n",
    "from distutils.version import LooseVersion\n",
    "\n",
    "\n",
    "# Preprocessing: create a Numpy array\n",
    "numpy_array = numpy.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=float)\n",
    "if LooseVersion(numpy.version.version) < LooseVersion('1.14'):\n",
    "    print('Original Numpy array:\\n{}\\n'.format(numpy.array2string(numpy_array)))\n",
    "else:\n",
    "    print('Original Numpy array:\\n{}\\n'.format(numpy.array2string(numpy_array, legacy='1.13')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorProto:\n",
      "dims: 2\n",
      "dims: 3\n",
      "data_type: 11\n",
      "raw_data: \"\\000\\000\\000\\000\\000\\000\\360?\\000\\000\\000\\000\\000\\000\\000@\\000\\000\\000\\000\\000\\000\\010@\\000\\000\\000\\000\\000\\000\\020@\\000\\000\\000\\000\\000\\000\\024@\\000\\000\\000\\000\\000\\000\\030@\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the Numpy array to a TensorProto\n",
    "tensor = numpy_helper.from_array(numpy_array)\n",
    "print('TensorProto:\\n{}'.format(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After round trip, Numpy array:\n",
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the TensorProto to a Numpy array\n",
    "new_array = numpy_helper.to_array(tensor)\n",
    "if LooseVersion(numpy.version.version) < LooseVersion('1.14'):\n",
    "    print('After round trip, Numpy array:\\n{}\\n'.format(numpy.array2string(numpy_array)))\n",
    "else:\n",
    "    print('After round trip, Numpy array:\\n{}\\n'.format(numpy.array2string(numpy_array, legacy='1.13')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TensorProto\n",
    "with open(os.path.join('resources', 'tensor.pb'), 'wb') as f:\n",
    "    f.write(tensor.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After saving and loading, new TensorProto:\n",
      "dims: 2\n",
      "dims: 3\n",
      "data_type: 11\n",
      "raw_data: \"\\000\\000\\000\\000\\000\\000\\360?\\000\\000\\000\\000\\000\\000\\000@\\000\\000\\000\\000\\000\\000\\010@\\000\\000\\000\\000\\000\\000\\020@\\000\\000\\000\\000\\000\\000\\024@\\000\\000\\000\\000\\000\\000\\030@\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorProto\n",
    "new_tensor = onnx.TensorProto()\n",
    "with open(os.path.join('resources', 'tensor.pb'), 'rb') as f:\n",
    "    new_tensor.ParseFromString(f.read())\n",
    "print('After saving and loading, new TensorProto:\\n{}'.format(new_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5、Creating an ONNX Model Using Helper Functions\n",
    "#### make_model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ir_version in model: 7\n",
      "\n",
      "The producer_name in model: onnx-example\n",
      "\n",
      "The graph in model:\n",
      "node {\n",
      "  input: \"X\"\n",
      "  input: \"Pads\"\n",
      "  output: \"Y\"\n",
      "  op_type: \"Pad\"\n",
      "  attribute {\n",
      "    name: \"mode\"\n",
      "    s: \"constant\"\n",
      "    type: STRING\n",
      "  }\n",
      "}\n",
      "name: \"test-model\"\n",
      "initializer {\n",
      "  dims: 4\n",
      "  data_type: 7\n",
      "  int64_data: 0\n",
      "  int64_data: 0\n",
      "  int64_data: 1\n",
      "  int64_data: 1\n",
      "  name: \"Pads\"\n",
      "}\n",
      "input {\n",
      "  name: \"X\"\n",
      "  type {\n",
      "    tensor_type {\n",
      "      elem_type: 1\n",
      "      shape {\n",
      "        dim {\n",
      "          dim_value: 1\n",
      "        }\n",
      "        dim {\n",
      "          dim_value: 2\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "input {\n",
      "  name: \"Pads\"\n",
      "  type {\n",
      "    tensor_type {\n",
      "      elem_type: 7\n",
      "      shape {\n",
      "        dim {\n",
      "          dim_value: 4\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "output {\n",
      "  name: \"Y\"\n",
      "  type {\n",
      "    tensor_type {\n",
      "      elem_type: 1\n",
      "      shape {\n",
      "        dim {\n",
      "          dim_value: 1\n",
      "        }\n",
      "        dim {\n",
      "          dim_value: 4\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "The model is checked!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import onnx\n",
    "from onnx import helper\n",
    "from onnx import AttributeProto, TensorProto, GraphProto\n",
    "\n",
    "\n",
    "# The protobuf definition can be found here:\n",
    "# https://github.com/onnx/onnx/blob/master/onnx/onnx.proto\n",
    "\n",
    "\n",
    "# Create one input (ValueInfoProto)\n",
    "X = helper.make_tensor_value_info('X', TensorProto.FLOAT, [1, 2])\n",
    "\n",
    "# Create second input (ValueInfoProto)\n",
    "Pads = helper.make_tensor_value_info('Pads', TensorProto.INT64, [4])\n",
    "\n",
    "# Create one output (ValueInfoProto)\n",
    "Y = helper.make_tensor_value_info('Y', TensorProto.FLOAT, [1, 4])\n",
    "\n",
    "# Create a node (NodeProto)\n",
    "node_def = helper.make_node(\n",
    "    'Pad', # node name\n",
    "    ['X', 'Pads'], # inputs\n",
    "    ['Y'], # outputs\n",
    "    mode='constant', # Attributes\n",
    ")\n",
    "\n",
    "# Create the graph (GraphProto)\n",
    "graph_def = helper.make_graph(\n",
    "    [node_def],\n",
    "    \"test-model\",\n",
    "    [X, Pads],\n",
    "    [Y],\n",
    "    [helper.make_tensor('Pads', TensorProto.INT64, [4,], [0, 0, 1, 1,])],\n",
    ")\n",
    "\n",
    "# Create the model (ModelProto)\n",
    "model_def = helper.make_model(graph_def,\n",
    "                              producer_name='onnx-example')\n",
    "\n",
    "print('The ir_version in model: {}\\n'.format(model_def.ir_version))\n",
    "print('The producer_name in model: {}\\n'.format(model_def.producer_name))\n",
    "print('The graph in model:\\n{}'.format(model_def.graph))\n",
    "onnx.checker.check_model(model_def)\n",
    "print('The model is checked!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Protobufs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Int attribute:\n",
      "\n",
      "name: \"this_is_an_int\"\n",
      "i: 1701\n",
      "type: INT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Int Attibute\n",
    "arg = helper.make_attribute(\"this_is_an_int\", 1701)\n",
    "print(\"\\nInt attribute:\\n\")\n",
    "print(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Float attribute:\n",
      "\n",
      "name: \"this_is_a_float\"\n",
      "f: 3.1410000324249268\n",
      "type: FLOAT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NBVAL_IGNORE_OUTPUT\n",
    "# Float Attribute\n",
    "arg = helper.make_attribute(\"this_is_a_float\", 3.141)\n",
    "print(\"\\nFloat attribute:\\n\")\n",
    "print(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String attribute:\n",
      "\n",
      "name: \"this_is_a_string\"\n",
      "s: \"string_content\"\n",
      "type: STRING\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# String Attribute\n",
    "arg = helper.make_attribute(\"this_is_a_string\", \"string_content\")\n",
    "print(\"\\nString attribute:\\n\")\n",
    "print(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Repeated int attribute:\n",
      "\n",
      "name: \"this_is_a_repeated_int\"\n",
      "ints: 1\n",
      "ints: 2\n",
      "ints: 1\n",
      "ints: 4\n",
      "type: INTS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Repeated Attribute\n",
    "arg = helper.make_attribute(\"this_is_a_repeated_int\", [1, 2, 1, 4])\n",
    "print(\"\\nRepeated int attribute:\\n\")\n",
    "print(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NodeProto:\n",
      "\n",
      "input: \"X\"\n",
      "output: \"Y\"\n",
      "op_type: \"Relu\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# node\n",
    "node_proto = helper.make_node(\"Relu\", [\"X\"], [\"Y\"])\n",
    "\n",
    "print(\"\\nNodeProto:\\n\")\n",
    "print(node_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NodeProto:\n",
      "\n",
      "input: \"X\"\n",
      "input: \"W\"\n",
      "input: \"B\"\n",
      "output: \"Y\"\n",
      "op_type: \"Conv\"\n",
      "attribute {\n",
      "  name: \"kernel\"\n",
      "  i: 3\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"pad\"\n",
      "  i: 1\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"stride\"\n",
      "  i: 1\n",
      "  type: INT\n",
      "}\n",
      "\n",
      "\n",
      "More Readable NodeProto (no args yet):\n",
      "\n",
      "%Y = Conv[kernel = 3, pad = 1, stride = 1](%X, %W, %B)\n"
     ]
    }
   ],
   "source": [
    "# node with args\n",
    "node_proto = helper.make_node(\n",
    "    \"Conv\", [\"X\", \"W\", \"B\"], [\"Y\"],\n",
    "    kernel=3, stride=1, pad=1)\n",
    "\n",
    "# This is just for making the attributes to be printed in order\n",
    "node_proto.attribute.sort(key=lambda attr: attr.name)\n",
    "print(\"\\nNodeProto:\\n\")\n",
    "print(node_proto)\n",
    "\n",
    "print(\"\\nMore Readable NodeProto (no args yet):\\n\")\n",
    "print(helper.printable_node(node_proto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "graph proto:\n",
      "\n",
      "node {\n",
      "  input: \"X\"\n",
      "  input: \"W1\"\n",
      "  input: \"B1\"\n",
      "  output: \"H1\"\n",
      "  op_type: \"FC\"\n",
      "}\n",
      "node {\n",
      "  input: \"H1\"\n",
      "  output: \"R1\"\n",
      "  op_type: \"Relu\"\n",
      "}\n",
      "node {\n",
      "  input: \"R1\"\n",
      "  input: \"W2\"\n",
      "  input: \"B2\"\n",
      "  output: \"Y\"\n",
      "  op_type: \"FC\"\n",
      "}\n",
      "name: \"MLP\"\n",
      "input {\n",
      "  name: \"X\"\n",
      "  type {\n",
      "    tensor_type {\n",
      "      elem_type: 1\n",
      "      shape {\n",
      "        dim {\n",
      "          dim_value: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "input {\n",
      "  name: \"W1\"\n",
      "  type {\n",
      "    tensor_type {\n",
      "      elem_type: 1\n",
      "      shape {\n",
      "        dim {\n",
      "          dim_value: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "input {\n",
      "  name: \"B1\"\n",
      "  type {\n",
      "    tensor_type {\n",
      "      elem_type: 1\n",
      "      shape {\n",
      "        dim {\n",
      "          dim_value: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "input {\n",
      "  name: \"W2\"\n",
      "  type {\n",
      "    tensor_type {\n",
      "      elem_type: 1\n",
      "      shape {\n",
      "        dim {\n",
      "          dim_value: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "input {\n",
      "  name: \"B2\"\n",
      "  type {\n",
      "    tensor_type {\n",
      "      elem_type: 1\n",
      "      shape {\n",
      "        dim {\n",
      "          dim_value: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "output {\n",
      "  name: \"Y\"\n",
      "  type {\n",
      "    tensor_type {\n",
      "      elem_type: 1\n",
      "      shape {\n",
      "        dim {\n",
      "          dim_value: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "More Readable GraphProto:\n",
      "\n",
      "graph MLP (\n",
      "  %X[FLOAT, 1]\n",
      "  %W1[FLOAT, 1]\n",
      "  %B1[FLOAT, 1]\n",
      "  %W2[FLOAT, 1]\n",
      "  %B2[FLOAT, 1]\n",
      ") {\n",
      "  %H1 = FC(%X, %W1, %B1)\n",
      "  %R1 = Relu(%H1)\n",
      "  %Y = FC(%R1, %W2, %B2)\n",
      "  return %Y\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# graph\n",
    "graph_proto = helper.make_graph(\n",
    "    [\n",
    "        helper.make_node(\"FC\", [\"X\", \"W1\", \"B1\"], [\"H1\"]),\n",
    "        helper.make_node(\"Relu\", [\"H1\"], [\"R1\"]),\n",
    "        helper.make_node(\"FC\", [\"R1\", \"W2\", \"B2\"], [\"Y\"]),\n",
    "    ],\n",
    "    \"MLP\",\n",
    "    [\n",
    "        helper.make_tensor_value_info('X' , TensorProto.FLOAT, [1]),\n",
    "        helper.make_tensor_value_info('W1', TensorProto.FLOAT, [1]),\n",
    "        helper.make_tensor_value_info('B1', TensorProto.FLOAT, [1]),\n",
    "        helper.make_tensor_value_info('W2', TensorProto.FLOAT, [1]),\n",
    "        helper.make_tensor_value_info('B2', TensorProto.FLOAT, [1]),\n",
    "    ],\n",
    "    [\n",
    "        helper.make_tensor_value_info('Y', TensorProto.FLOAT, [1]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\ngraph proto:\\n\")\n",
    "print(graph_proto)\n",
    "\n",
    "print(\"\\nMore Readable GraphProto:\\n\")\n",
    "print(helper.printable_graph(graph_proto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NodeProto that contains a graph:\n",
      "\n",
      "input: \"Input\"\n",
      "input: \"W1\"\n",
      "input: \"B1\"\n",
      "input: \"W2\"\n",
      "input: \"B2\"\n",
      "output: \"Output\"\n",
      "op_type: \"graph\"\n",
      "attribute {\n",
      "  name: \"graph\"\n",
      "  graphs {\n",
      "    node {\n",
      "      input: \"X\"\n",
      "      input: \"W1\"\n",
      "      input: \"B1\"\n",
      "      output: \"H1\"\n",
      "      op_type: \"FC\"\n",
      "    }\n",
      "    node {\n",
      "      input: \"H1\"\n",
      "      output: \"R1\"\n",
      "      op_type: \"Relu\"\n",
      "    }\n",
      "    node {\n",
      "      input: \"R1\"\n",
      "      input: \"W2\"\n",
      "      input: \"B2\"\n",
      "      output: \"Y\"\n",
      "      op_type: \"FC\"\n",
      "    }\n",
      "    name: \"MLP\"\n",
      "    input {\n",
      "      name: \"X\"\n",
      "      type {\n",
      "        tensor_type {\n",
      "          elem_type: 1\n",
      "          shape {\n",
      "            dim {\n",
      "              dim_value: 1\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    input {\n",
      "      name: \"W1\"\n",
      "      type {\n",
      "        tensor_type {\n",
      "          elem_type: 1\n",
      "          shape {\n",
      "            dim {\n",
      "              dim_value: 1\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    input {\n",
      "      name: \"B1\"\n",
      "      type {\n",
      "        tensor_type {\n",
      "          elem_type: 1\n",
      "          shape {\n",
      "            dim {\n",
      "              dim_value: 1\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    input {\n",
      "      name: \"W2\"\n",
      "      type {\n",
      "        tensor_type {\n",
      "          elem_type: 1\n",
      "          shape {\n",
      "            dim {\n",
      "              dim_value: 1\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    input {\n",
      "      name: \"B2\"\n",
      "      type {\n",
      "        tensor_type {\n",
      "          elem_type: 1\n",
      "          shape {\n",
      "            dim {\n",
      "              dim_value: 1\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    output {\n",
      "      name: \"Y\"\n",
      "      type {\n",
      "        tensor_type {\n",
      "          elem_type: 1\n",
      "          shape {\n",
      "            dim {\n",
      "              dim_value: 1\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  type: GRAPHS\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# An node that is also a graph\n",
    "graph_proto = helper.make_graph(\n",
    "    [\n",
    "        helper.make_node(\"FC\", [\"X\", \"W1\", \"B1\"], [\"H1\"]),\n",
    "        helper.make_node(\"Relu\", [\"H1\"], [\"R1\"]),\n",
    "        helper.make_node(\"FC\", [\"R1\", \"W2\", \"B2\"], [\"Y\"]),\n",
    "    ],\n",
    "    \"MLP\",\n",
    "    [\n",
    "        helper.make_tensor_value_info('X' , TensorProto.FLOAT, [1]),\n",
    "        helper.make_tensor_value_info('W1', TensorProto.FLOAT, [1]),\n",
    "        helper.make_tensor_value_info('B1', TensorProto.FLOAT, [1]),\n",
    "        helper.make_tensor_value_info('W2', TensorProto.FLOAT, [1]),\n",
    "        helper.make_tensor_value_info('B2', TensorProto.FLOAT, [1]),\n",
    "    ],\n",
    "    [\n",
    "        helper.make_tensor_value_info('Y', TensorProto.FLOAT, [1]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# output = ThisSpecificgraph([input, w1, b1, w2, b2])\n",
    "node_proto = helper.make_node(\n",
    "    \"graph\",\n",
    "    [\"Input\", \"W1\", \"B1\", \"W2\", \"B2\"],\n",
    "    [\"Output\"],\n",
    "    graph=[graph_proto],\n",
    ")\n",
    "\n",
    "print(\"\\nNodeProto that contains a graph:\\n\")\n",
    "print(node_proto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6、Checking an ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import onnx\n",
    "import os\n",
    "\n",
    "\n",
    "# Preprocessing: load the ONNX model\n",
    "model_path = os.path.join('resources', 'alexnet.onnx')\n",
    "onnx_model = onnx.load(model_path)\n",
    "\n",
    "print('The model is:\\n{}'.format(onnx_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is checked!\n"
     ]
    }
   ],
   "source": [
    "# Check the model\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print('The model is checked!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7、Checking a Large ONNX Model >2GB\n",
    "Current checker supports checking models with external data, but for those models larger than 2GB, please use the model path for onnx.checker and the external data needs to be under the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# onnx.checker.check_model('path/to/the/model.onnx')\n",
    "# onnx.checker.check_model(loaded_onnx_model) will fail if given >2GB model\n",
    "onnx.checker.check_model('resources/alexnet.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8、Optimizing an ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model before optimization:\n",
      "\n",
      "graph two-transposes (\n",
      "  %X[FLOAT, 2x3x4]\n",
      ") {\n",
      "  %Y = Transpose[perm = [1, 0, 2]](%X)\n",
      "  %Z = Transpose[perm = [1, 0, 2]](%Y)\n",
      "  return %Z\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import onnx\n",
    "import os\n",
    "from onnx import optimizer\n",
    "\n",
    "# Preprocessing: load the model contains two transposes.\n",
    "model_path = os.path.join('resources', 'two_transposes.onnx')\n",
    "original_model = onnx.load(model_path)\n",
    "\n",
    "print('The model before optimization:\\n\\n{}'.format(onnx.helper.printable_graph(original_model.graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available optimization passes:\n",
      "\teliminate_deadend\n",
      "\teliminate_identity\n",
      "\teliminate_nop_dropout\n",
      "\teliminate_nop_monotone_argmax\n",
      "\teliminate_nop_pad\n",
      "\teliminate_nop_transpose\n",
      "\teliminate_unused_initializer\n",
      "\textract_constant_to_initializer\n",
      "\tfuse_add_bias_into_conv\n",
      "\tfuse_bn_into_conv\n",
      "\tfuse_consecutive_concats\n",
      "\tfuse_consecutive_log_softmax\n",
      "\tfuse_consecutive_reduce_unsqueeze\n",
      "\tfuse_consecutive_squeezes\n",
      "\tfuse_consecutive_transposes\n",
      "\tfuse_matmul_add_bias_into_gemm\n",
      "\tfuse_pad_into_conv\n",
      "\tfuse_transpose_into_gemm\n",
      "\tlift_lexical_references\n",
      "\tnop\n",
      "\tsplit_init\n",
      "\tsplit_predict\n",
      "end!\n",
      "The model after optimization:\n",
      "\n",
      "graph two-transposes (\n",
      "  %X[FLOAT, 2x3x4]\n",
      ") {\n",
      "  %Z = Transpose[perm = [0, 1, 2]](%X)\n",
      "  return %Z\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# A full list of supported optimization passes can be found using get_available_passes()\n",
    "all_passes = optimizer.get_available_passes()\n",
    "print(\"Available optimization passes:\")\n",
    "for p in all_passes:\n",
    "    print('\\t{}'.format(p))\n",
    "print(\"end!\")\n",
    "\n",
    "# Pick one pass as example\n",
    "passes = ['fuse_consecutive_transposes']\n",
    "\n",
    "# Apply the optimization on the original serialized model\n",
    "optimized_model = optimizer.optimize(original_model, passes)\n",
    "\n",
    "print('The model after optimization:\\n\\n{}'.format(onnx.helper.printable_graph(optimized_model.graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9、Running Shape Inference on an ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before shape inference, the shape info of Y is:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import onnx\n",
    "from onnx import helper, shape_inference\n",
    "from onnx import TensorProto\n",
    "\n",
    "\n",
    "# Preprocessing: create a model with two nodes, Y's shape is unknown\n",
    "node1 = helper.make_node('Transpose', ['X'], ['Y'], perm=[1, 0, 2])\n",
    "node2 = helper.make_node('Transpose', ['Y'], ['Z'], perm=[1, 0, 2])\n",
    "\n",
    "graph = helper.make_graph(\n",
    "    [node1, node2],\n",
    "    'two-transposes',\n",
    "    [helper.make_tensor_value_info('X', TensorProto.FLOAT, (2, 3, 4))],\n",
    "    [helper.make_tensor_value_info('Z', TensorProto.FLOAT, (2, 3, 4))],\n",
    ")\n",
    "\n",
    "original_model = helper.make_model(graph, producer_name='onnx-examples')\n",
    "\n",
    "# Check the model and print Y's shape information\n",
    "onnx.checker.check_model(original_model)\n",
    "print('Before shape inference, the shape info of Y is:\\n{}'.format(original_model.graph.value_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After shape inference, the shape info of Y is:\n",
      "[name: \"Y\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_value: 3\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 2\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 4\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Apply shape inference on the model\n",
    "inferred_model = shape_inference.infer_shapes(original_model)\n",
    "\n",
    "# Check the model and print Y's shape information\n",
    "onnx.checker.check_model(inferred_model)\n",
    "print('After shape inference, the shape info of Y is:\\n{}'.format(inferred_model.graph.value_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10、Converting Version of an ONNX Model within Default Domain (\"\"/\"ai.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Input features.0.weight is undefined!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-076b72b0c271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Apply the version conversion on the original model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# converted_model = version_converter.convert_version(original_model, <int target_version>)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mconverted_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mversion_converter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finished\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# print('The model after conversion:\\n{}'.format(converted_model))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/onnx-1.7.0-py3.6-linux-x86_64.egg/onnx/version_converter.py\u001b[0m in \u001b[0;36mconvert_version\u001b[0;34m(model, target_version)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'VersionConverter only accepts int as target_version, incorrect type: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mmodel_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mconverted_model_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted_model_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Input features.0.weight is undefined!"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx import version_converter, helper\n",
    "\n",
    "# Preprocessing: load the model to be converted.\n",
    "model_path = 'resources/alexnet2.onnx'\n",
    "original_model = onnx.load(model_path)\n",
    "\n",
    "# print('The model before conversion:\\n{}'.format(original_model))\n",
    "\n",
    "# A full list of supported adapters can be found here:\n",
    "# https://github.com/onnx/onnx/blob/master/onnx/version_converter.py#L21\n",
    "# Apply the version conversion on the original model\n",
    "# converted_model = version_converter.convert_version(original_model, <int target_version>)\n",
    "converted_model = version_converter.convert_version(original_model, 11)\n",
    "print(\"finished\")\n",
    "# print('The model after conversion:\\n{}'.format(converted_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "### 1、Polishing the Model\n",
    "\n",
    "Function polish_model runs model checker, optimizer, shape inference engine on the model, and also strips the doc_string for you.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "dummy_input = torch.randn(10, 3, 224, 224)\n",
    "model = torchvision.models.alexnet(pretrained=True)\n",
    "\n",
    "torch.onnx.export(model, dummy_input, \"resources/alexnet2.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Input features.0.weight is undefined!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1850ff624711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resources/alexnet2.onnx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpolished_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolish_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/onnx-1.7.0-py3.6-linux-x86_64.egg/onnx/utils.py\u001b[0m in \u001b[0;36mpolish_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip_doc_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_inference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/onnx-1.7.0-py3.6-linux-x86_64.egg/onnx/optimizer.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(model, passes, fixed_point)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0moptimized_model_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_fixedpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0moptimized_model_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimized_model_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Input features.0.weight is undefined!"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnx.utils\n",
    "\n",
    "\n",
    "model = onnx.load('resources/alexnet2.onnx')\n",
    "polished_model = onnx.utils.polish_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Input features.0.weight is undefined!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a43792f278f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minferred_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_inference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minferred_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fuse_bn_into_conv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/onnx-1.7.0-py3.6-linux-x86_64.egg/onnx/optimizer.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(model, passes, fixed_point)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0moptimized_model_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_fixedpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0moptimized_model_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimized_model_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Input features.0.weight is undefined!"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnx.optimizer\n",
    "model = onnx.load(\"resources/alexnet2.onnx\")\n",
    "onnx.checker.check_model(model)\n",
    "inferred_model = onnx.shape_inference.infer_shapes(model)\n",
    "model = onnx.optimizer.optimize(inferred_model, passes=['fuse_bn_into_conv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Updating Model's Inputs Outputs Dimension Sizes with Variable Length\n",
    "\n",
    "Function update_inputs_outputs_dims updates the dimension of the inputs and outputs of the model, to the provided values in the parameter. You could provide both static and dynamic dimension size, by using dim_param. For more information on static and dynamic dimension size, checkout Tensor Shapes.\n",
    "\n",
    "The function runs model checker after the input/output sizes are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input.1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-658c538d2d93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resources/alexnet2.onnx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Here both 'seq', 'batch' and -1 are dynamic using dim_param.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mvariable_length_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_model_dims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_inputs_outputs_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'input_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'seq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'output_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'seq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/onnx-1.7.0-py3.6-linux-x86_64.egg/onnx/tools/update_model_dims.py\u001b[0m in \u001b[0;36mupdate_inputs_outputs_dims\u001b[0;34m(model, input_dims, output_dims)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0minput_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0minput_dim_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mupdate_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input.1'"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx.tools import update_model_dims\n",
    "\n",
    "model = onnx.load('resources/alexnet2.onnx')\n",
    "# Here both 'seq', 'batch' and -1 are dynamic using dim_param.\n",
    "variable_length_model = update_model_dims.update_inputs_outputs_dims(model, {'input_name': ['seq', 'batch', 3, -1]}, {'output_name': ['seq', 'batch', 1, -1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
